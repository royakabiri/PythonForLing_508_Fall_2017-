{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG align=left src=\"https://faculty.sbs.arizona.edu/hammond/ling508-f17/logo.png\" height=30>\n",
    "\n",
    "<div align=right>\n",
    "Linguistics 408/508<br>\n",
    "Hammond<br>\n",
    "Fall 2017\n",
    "</div>\n",
    "\n",
    "<h1 align=center>Homework #5</h1>\n",
    "\n",
    "## General\n",
    "\n",
    "<ol>\n",
    "\n",
    "<li>This is due at the <i>beginning</i> of class on <b>Oct. 31</b>.\n",
    "\n",
    "<li>Rename this file based on your last name, e.g. <code>hw5.ipynb</code> &rarr; <code>jones5.ipynb</code>\n",
    "\n",
    "<li>Answer all questions in code or text/markdown boxes and/or\n",
    "in separate files as specified by the question.\n",
    "\n",
    "<li>Code does not need to be perfect, but make sure it <strong>runs without error</strong>.\n",
    "\n",
    "<li>Make sure to <i>save</i> the file....\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 408 and 508\n",
    "\n",
    "<ol>\n",
    "\n",
    "<li>Give regular expressions for the following patterns:\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "lines = []\n",
    "f = open('alice.txt','r')\n",
    "t = f.read()\n",
    "f.close()\n",
    "#rk:I included all punctuation to be replaced by spaces\n",
    "t = re.sub('[\\.\\?\\-!\\?\\*,\"\\(\\):\\`\\[\\];_/~]',' ',t) \n",
    "words = t.split()\n",
    "\n",
    "pats = []\n",
    "\n",
    "#matches any adverb\n",
    "#rk:overreports cases like \"apply\" \n",
    "#rk:got rid of cases like \"fly\" \n",
    "pats.append('^\\w\\w+ly$')\n",
    "\n",
    "\n",
    "#matches any past tense verb\n",
    "#rk:overreport \"hundred\"\n",
    "#rk:got rid of \"need\", \"bed\", \"red\" by specify that\n",
    "#rk:base form should have at least 3 characters ('^\\w\\w\\w+ed$')\n",
    "#rk:but it excluded \"used\" and \"tied\", so I ended up not\n",
    "#rk:using it\n",
    "pats.append('^\\w+ed$') \n",
    "\n",
    "#matches any word that begins with three consonants\n",
    "#(treat things like th, sh, ch as single consonants)\n",
    "#rk: based on the English phonotactics, \n",
    "#rk:just some specific sounds can\n",
    "#rk: co-occur in consonant clusters.\n",
    "#rk:if not aware of the phonotactics, could just write :\n",
    "#'^[bcdfghjklmnpqrstvwxz][bcdfgjklmnpqrstvwxz][bcdfgjklmnpqrstvwxz][aeiou][^aeiou]*[aeiou]*[^aeiou]*$'\n",
    "#rk: 'h' was excluded from the 2nd and 3rd consonant lists\n",
    "pats.append('^s[ctp][rl][aeiou][^aeiou]*[aeiou]*[^aeiou]*[aeiou]*[^aeiou]*$')\n",
    "\n",
    "\n",
    "#matches two syllable words   \n",
    "#(allow for diphthongs and final silent e)\n",
    "#rk:I tried to write a regex for C*VC*VC* to include all the \n",
    "#rk:possible two syllable patterns but I got lots of noises\n",
    "#rk:so I limited my regex. E.g, this excludes the CVCV patterns\n",
    "#rk:and overreports cases like 'readable'\n",
    "#rk:because of including final silent e'\n",
    "pats.append('^[^AaEeIiOoUu]+[aeiou][aeiou]*[^aeiou]+[aeiou][^aeiou]+e*$')\n",
    "            \n",
    "\n",
    "n = int(input('Enter a number (0-3)'))\n",
    "\n",
    "i = 0\n",
    "for word in words:\n",
    "    if re.search(pats[n],word):\n",
    "        i += 1\n",
    "        print(i,word)\n",
    "    if i > 11:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ol start=2>\n",
    "\n",
    "<li>This question involves doing some simple lexical statistics:\n",
    "\n",
    "<ol>\n",
    "<li>read in the <code>alice.txt</code> file and strip off the header;\n",
    "<li>normalize by replacing all punctuation with spaces and converting everything into lower case;\n",
    "<li>find anything that could be the past tense of a weak verb, i.e. anything that ends in <em>-ed</em>;\n",
    "<li>create verb stems by stripping off the past tense marker, being mindful of the spelling conventions that apply;\n",
    "<li>compare the candidate stems with all the <em>other</em> words in the corpus and print out any stems that do <em>not</em> occur independently in the corpus. (These should be mostly things that aren't really past tenses.)\n",
    "</ol>\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re,sys\n",
    "#rk:read the text file\n",
    "alice = open('alice.txt','r')\n",
    "aliceText = alice.read()\n",
    "alice.close()\n",
    "aliceText = aliceText[10900:]\n",
    "\n",
    "#rk:convert to lower case\n",
    "lowerAliceText = aliceText.lower()\n",
    "#rk:replace all punctuation with spaces\n",
    "newAliceText= re.sub('[\\.\\?\\-!\\?\\*,\"\\(\\):\\`\\[\\];_/~]',' ',lowerAliceText)\n",
    "#print(newAliceText)\n",
    "\n",
    "aliceWords = newAliceText.split()\n",
    "#print (aliceWords)\n",
    "\n",
    "#rk:find any past tense verb\n",
    "#rk:and strip off 'ed' simultaneously\n",
    "#rk:create two empty lists\n",
    "#rk:to store the past tense verbs\n",
    "#rk:and their stems separately\n",
    "pastTenseverbs=[]\n",
    "stemList=[]\n",
    "#rk:the word should have at least 3 characters\n",
    "#rk:to get rid of cases like 'red,bed,...'\n",
    "pattern1=('^((\\w\\w\\w+)ed)$') \n",
    "for word in aliceWords:\n",
    "    m =re.search(pattern1, word)\n",
    "    if m:\n",
    "        pastTenseverbs.append(m.group(1))\n",
    "        stemList.append(m.group(2))\n",
    "#print (pastTenseverbs)\n",
    "#print (stemList)\n",
    "\n",
    "#rk: tried to fix spelling conventions\n",
    "#rk: after striping off 'ed'\n",
    "#rk:I could not fix all of them \n",
    "#rk:still there are lots of noises in my output\n",
    "\n",
    "#rk: find any stems ended in i\n",
    "pat1='^(\\w+)(i)$'\n",
    "\n",
    "#rk: find any stems ended in double [prdt]\n",
    "#rk:also used 'l' in the list \n",
    "#rk:but it wrongly included 'fall, fill, pull'\n",
    "#rk:so I removed it\n",
    "pat2='^\\w\\w+([prdt])\\\\1$'\n",
    "\n",
    "#rk: find any stems which ended in 'l' \n",
    "#rk: preceded by a consonant  \n",
    "pat3='^\\w+[^aeioulrw]l$'\n",
    "\n",
    "#rk:tried hard to find a way to add 'e' to cases\n",
    "#rk:like 'pictur, confus, believ, involv, clos'\n",
    "#rk:but could not come up with any regex.\n",
    "#rk:most of them wrongly applied on many other cases \n",
    "\n",
    "#rk:create an empty list to store modified stems\n",
    "newStemList=[]\n",
    "#rk:for each word in the list\n",
    "for word in stemList:\n",
    "    m1=re.search(pat1, word)\n",
    "    m2=re.search(pat2, word)\n",
    "    m3=re.search(pat3, word)\n",
    "    \n",
    "    if m1:\n",
    "        #rk:replace i by y \n",
    "        newWord1=re.sub ('i','y',m1.group())\n",
    "        newStemList.append(newWord1)\n",
    "#print (newStemList)\n",
    "\n",
    "    elif m2:\n",
    "        #rk: remove the second 'p,r,d,t' at the end of the stems\n",
    "        newStemList.append(m2.group()[:-1])\n",
    "#print (newStemList)\n",
    "\n",
    "    elif m3:\n",
    "        #rk:add an 'e' to the end of the stems\n",
    "        newWord2=m3.group()+'e'\n",
    "        newStemList.append(newWord2)\n",
    "        \n",
    "    else:\n",
    "        #rk:if none, just append the word to the list\n",
    "        newStemList.append(word)\n",
    "#print (newStemList)\n",
    "\n",
    "#rk:create another empty list to store \n",
    "#rk:the stems not occur independently in the text\n",
    "finalStemList=[]\n",
    "for stem in newStemList:\n",
    "    #rk:if in the text, just skip it\n",
    "    if stem in aliceWords:\n",
    "        continue\n",
    "    #rk:if not in the text,\n",
    "    if stem not in aliceWords:\n",
    "        #rk:and if not in the final list\n",
    "        if stem not in finalStemList:\n",
    "            finalStemList.append(stem)\n",
    "count=1            \n",
    "for i in finalStemList:\n",
    "    print(count, i)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol start=3>\n",
    "\n",
    "<li>Read in a web page of your choosing, normalize, do <em>not</em> strip HTML, and print out the 10 most frequent words.\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request, re\n",
    "\n",
    "#rk:read in the linguistics webpage in wikipedia\n",
    "f = urllib.request.urlopen('https://en.wikipedia.org/wiki/Linguistics')\n",
    "b = f.read()\n",
    "f.close()\n",
    "t = b.decode('UTF-8')\n",
    "#print(t)\n",
    "\n",
    "#rk:convert the text into lowercase\n",
    "lowertext= t.lower()\n",
    "#print(lowertext)\n",
    "\n",
    "\n",
    "#rk:replace all punctuation and numbers with spaces\n",
    "newlowertext= re.sub('[\\.\\?\\-!\\?\\*,<>\"\\(\\):\\`\\[\\];_/~]',' ',lowertext)\n",
    "finallowertext= re.sub ('[0-9]', ' ', newlowertext)\n",
    "#print (finallowertext)\n",
    "\n",
    "\n",
    "#rk:split text into words\n",
    "textWords= finallowertext.split()\n",
    "#print (textWords)\n",
    "\n",
    "#rk:count frequency of words\n",
    "wordCountDict=  {}\n",
    "for w in textWords:\n",
    "    if w in wordCountDict:\n",
    "         wordCountDict[w]+=1\n",
    "    else:\n",
    "         wordCountDict[w]= 1\n",
    "#print (wordCountDict)\n",
    "\n",
    "\n",
    "#rk:sort it which makes it as a list of tuples  \n",
    "sortedList = sorted(wordCountDict.items(), key=lambda x: x[1], reverse=True)\n",
    "#print (sortedList)\n",
    "\n",
    "#rk:convert it back to a dictionary\n",
    "finalDict = dict(sortedList)\n",
    "#print (finalDict) \n",
    "\n",
    "#rk:Print out the 10 most frequent words and their counts.\n",
    "print (\"Ten most frequent words are as follows: \")  \n",
    "count = 1\n",
    "for key in finalDict:\n",
    "    if count < 11:\n",
    "           print (count, key, \":\", finalDict[key])\n",
    "    count += 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 508 only\n",
    "\n",
    "<ol start=4>\n",
    "\n",
    "<li>Do the same thing as above, except strip out HTML <em>with your own code</em>.<p>\n",
    "\n",
    "(How does this affect what the 10 words are?)\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rk: When I stripped out the HTML, some of those 10 words I got in the last question disappeared. \n",
    "Some words used in HTML (such as li, href, title, a, span, ...) have a high frequency. Thus, when the text included those, the code will also count them as words. But as soon as they are stripped out, the code will just count the frequency of the text words (which are also visible to us in the corpus). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request, re\n",
    "\n",
    "#rk:read in the linguistics webpage in wikipedia\n",
    "f = urllib.request.urlopen('https://en.wikipedia.org/wiki/Linguistics')\n",
    "b = f.read()\n",
    "f.close()\n",
    "t = b.decode('UTF-8')\n",
    "#print(t)\n",
    "\n",
    "#rk:convert the text into lowercase\n",
    "lowertext= t.lower()\n",
    "#print(lowertext)\n",
    "\n",
    "#rk:deleting everything up to the body\n",
    "lowertext= re.sub('^.*<body>','',lowertext,flags=re.S)\n",
    "\n",
    "\n",
    "#rk:get rid of html comments \n",
    "lowertext= re.sub('<!--[^-]*-->','',lowertext,flags=re.S)\n",
    "\n",
    "#rk:replace all tags with spaces\n",
    "#rk:I used the non-greedgy ? after * \n",
    "#rk: to capture the minimal match\n",
    "#rk:and exclude cases where there \n",
    "#rk:are embedded angle brackets\n",
    "#rk:but could also use this regex:\n",
    "#'<[^>]*>' ([^>]: anything other than >)\n",
    "\n",
    "lowertext = re.sub('<.*?>',' ',lowertext,flags=re.S)\n",
    "#print(lowertext)\n",
    "\n",
    "#rk:replace all punctuation and numbers with spaces\n",
    "newlowertext= re.sub('[\\.\\?\\-!\\?\\*,<>\"\\(\\):\\`\\[\\];_/~]',' ',lowertext)\n",
    "finallowertext= re.sub ('[0-9]', ' ', newlowertext)\n",
    "#print (finallowertext)\n",
    "\n",
    "\n",
    "#rk:split text into words\n",
    "textWords= finallowertext.split()\n",
    "#print (textWords)\n",
    "\n",
    "#rk:count frequency of words\n",
    "wordCountDict=  {}\n",
    "for w in textWords:\n",
    "    if w in wordCountDict:\n",
    "         wordCountDict[w]+=1\n",
    "    else:\n",
    "         wordCountDict[w]= 1\n",
    "#print (wordCountDict)\n",
    "\n",
    "\n",
    "#rk:sort it which makes it as a list of tuples  \n",
    "sortedList = sorted(wordCountDict.items(), key=lambda x: x[1], reverse=True)\n",
    "#print (sortedList)\n",
    "\n",
    "#rk:convert it back to a dictionary\n",
    "finalDict = dict(sortedList)\n",
    "#print (finalDict) \n",
    "\n",
    "#rk:Print out the 10 most frequent words and their counts.\n",
    "print (\"Ten most frequent words are as follows: \")  \n",
    "count = 1\n",
    "for key in finalDict:\n",
    "    if count < 11:\n",
    "           print (count, key, \":\", finalDict[key])\n",
    "    count += 1\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
